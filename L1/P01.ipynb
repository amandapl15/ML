{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hnMagog7C_x-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# <center> Project 01: Machine Learning for customer segmentation through clustering</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Hzjopj0C_yB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Seminar goals:\n",
    "In this project, we will develop and apply different clustering tecnhiques. Unsupervised methodologies are broadly used in business to generate **insights** from several type of data sources as text or image and they are a baseline tool for multiple areas marketing, sales or customer care. \n",
    "\n",
    "In our case, we will use clustering techniques to understand a customer base; i.e. describe the main characteristics of our customers for **marketing purposes**.\n",
    "\n",
    "During this project we will follow the end-to-end Machine Learning process: from data gathering and cleaning, exploratory data analysis, feature engineering and finally, training and prediction.\n",
    "The project is structured as follows:\n",
    "\n",
    "![<framework_ml_project>](framework_ml_project.png)\n",
    "\n",
    "\n",
    "In this project, we will apply two clustering techniques: K-means and Mixture of Gaussians. Both techniques follow different approach to cluster or segment the data. We will learn how to apply them using Python and how they are used to generate insights about our customer base, i.e. identify the main types or **sterotypes** of customers and their differences. Besides, we will learn to calculate the optimal K value and measure the quality of the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ThyIMxSgC_yC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Due date: up to May, 20th, 23:55h. \n",
    "### Submission procedure: via Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cn_5QIADC_yD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*******"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "euwcRZfAC_yE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 0. Understanding the problem: customers' stereotypes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMZkBnEjC_yF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Recently, we have joined a data scientist and AI engineering team of a telco company. This team is supporting decision making process of several internal areas as marketing and customer care.\n",
    "\n",
    "Our marketing colleagues are planning to launch a new commercial campaign for a new mobile tariff. As input for the tariff design, the Marketing product managers needs to know the **pattern** or **stereotypes** of our current customers: i.e. the main customers' sectors (Industry, Services, Agriculture, ...), number of employees, Turnover, which products they consume, how much they spend in telco services, etc. \n",
    "\n",
    "To support marketing demands, we are going **to use two of the most important clustering techniques: K-means and Mixture of Gaussians (MoG).**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMD1CqboC_yG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 1: Data gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WBCPMk53C_yH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "\n",
    "![<data_gathering>](data_gathering.png)\n",
    "\n",
    "\n",
    "In this practice we are using a new dataset named `customer_dt.csv`. This file contains information of **272 of our telco's consumers** for 3 months. In particular, the detailed information for each customer is:\n",
    "\n",
    "- *Company_id*: It's an integer that identifies any company. Each value is unique for every company.\n",
    "- *Reference_date_month*: It refers to the month corresponding to the customer information\n",
    "- *Product*: Name level 1 of the product\n",
    "- *Sub_product*: Name level 2 of the product\n",
    "- *Type_ID_Coverage_GSM*: type of coverage for GSM (2G) network: indoor, outdoor or no-coverage\n",
    "- *Type_ID_Coverage_UMTS*: type of coverage for UMTS (3G) network: indoor, outdoor or no-coverage\n",
    "- *Type_ID_Coverage_LTE*: type of coverage for LTE (4G) network: indoor, outdoor or no-coverage\n",
    "- *CNT_EMPLOYEE: Number of employees of the company\n",
    "- *Sector*: It's an integer that identifies the sector of the company's activity\n",
    "- *Sub_sector*: It's an integer that identifies the sub_sector of the company's activity\n",
    "- *Turnover*: The annual incomes of the company\n",
    "- *ZIP*: Postal code where the company is placed\n",
    "- *Data_usage*: Number of Gigabits for this product and sub_product and Reference_date_month \n",
    "- *Voice_usage*: Number of minutes for this product and sub_product and Reference_date_month \n",
    "- *Monthly_expense*: Euros expended in this product and sub_product and Reference_date_month\n",
    "- *N_lines*: Number of mobile lines this product and sub_product and Reference_date_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lD22OaRsC_yN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 2: Data understanding and preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZ6q20FUC_yO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once we know the problem to solve, the next stage is to have a clear understanding of the data we have extracted and to prepare it before modelling. In particular, we will:\n",
    "- List and verify the type of each variable (object, float, int...). Identify variables with nulls. Measure the memory usage\n",
    "- Eliminate rows with nulls in order to have a dataset 100% fulfilled\n",
    "- Aggregate rows with monthly expense per customers in order to have just 1 sample per customers\n",
    "- Exploratory Data Analysis to understand main statistics (mean, standard deviation, min&max values and 25%-50%-75% quartiles) and distribution of the most relevant variables or features as data usage, voice usage, monthly expense and number of lines\n",
    "- Plot several graphs in order to identify how variables are related between them. In particular:\n",
    "- correlation matrix\n",
    "- 2D and 3D scatter plots between data usage, voice usage and monthly expense\n",
    "\n",
    "Once this part, also known as **data wrangling** of the Project is done, we should achieve a deep knowledge about the data. Besides, the dataset will have been processed to be ready to apply the clustering algorithms to solve the business problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's import the main Python libraries required in our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import pyplot\n",
    "from mpl_toolkits import mplot3d\n",
    "from scipy.stats import chi2_contingency\n",
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from matplotlib.patches import Ellipse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5lfYuTUC_yQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Open the csv with separator \";\" and assign to a dataframe variable (use read_csv from Pandas library). Let's see the top 5 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I9VU_uEJC_yR",
    "outputId": "5a1ad7e2-48de-435c-aba6-fb8b1b9f367f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "customer_dt = pd.read_csv('customer_dt.csv', sep=\";\")\n",
    "customer_dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1rjGZecC_yW",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX1**] [**REPORT**] Let's identify the type of the variables (integer, float, chart...) and the size of the dataset and the file. Which is the variable with more nulls? And with no nulls? \n",
    "\n",
    "Tip: [.info()](https://www.geeksforgeeks.org/python-pandas-dataframe-info/) is a function that reports the main characteristics of a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "customer_dt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qU8i852UC_ya",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In our dataset there a subset of variables that describe a company regardless of the month, i.e. `Company_id`, `CNT_EMPLOYEE`, `Sector`, `Sub_sector`, `Turnover`, `Revenue_Scale` and `ZIP`. We should guarantee that our dataset for training the cluster has no **nulls** in those variables.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_rz7xIchC_yb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX2**] [**CODE**] Eliminate those rows with nulls in any of the descriptive company variables, i.e. `Company_id`, `CNT_EMPLOYEE`, `Sector`, `Sub_sector`, `Turnover`, `Revenue_Scale` and `ZIP`.\n",
    "\n",
    "Tip: you may have to use the [set_index](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.set_index.html) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Usaremos el codigo del tutorial como referencia\n",
    "print(\"Removing nulls\\n\")\n",
    "customer_dt=customer_dt[(customer_dt['Company_id'].isna()==False) & (customer_dt['CNT_EMPLOYEE'].isna()==False) \n",
    "                        & (customer_dt['Sector'].isna()==False) & (customer_dt['Sub_sector'].isna()==False)& (customer_dt['Turnover'].isna()==False)& (customer_dt['Revenue_Scale'].isna()==False)& (customer_dt['ZIP'].isna()==False)]\n",
    "print(\"The resulted lenght is:\", len(customer_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eT1O_XmsC_ye",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's re-calculate the type of the variables (integer, float, chart...) and the size of the dataset and the file. Your output should look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZW-UMuBVC_yf",
    "outputId": "6af1db09-dff9-4870-ade6-c41db35fea83",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "customer_dt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2oi7OYCC_yi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we observe, there are still few rows with **null** values in `TYPE_ID_Coverage_GSM`, `TYPE_ID_Coverage_UMTS` and `TYPE_ID_Coverage_LTE`. Let's understand which registers are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nSqGLF6vC_yj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX3**] [**CODE**] Create a `non_coverage_register` variable with the rows with **nulls** in these coverage variables. How many registers does the new `non_coverage_register` variable have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#De forma muy parecida al ejercicio 2, usamos como referencia lo visto en el tutorial\n",
    "non_coverage_register = customer_dt[(customer_dt['TYPE_ID_Coverage_GSM'].isna()==True)&(customer_dt['TYPE_ID_Coverage_UMTS'].isna()==True)&(customer_dt['TYPE_ID_Coverage_LTE'].isna()==True)]\n",
    "\n",
    "print(\"The resulted lenght is:\", len(non_coverage_register))\n",
    "non_coverage_register"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z-c-nLT4C_yn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As **nulls** in coverage only affects one customer, we can decide to remove it from the training dataset or to impute an statistic value as the mean, median or most frequent value in each of these nulls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07JpDlFqC_yo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX4**] [**CODE**] Remove the customers with **nulls** in coverage variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Lo mismo que en el ejercicio dos pero para estas variables que aun tienen nulos\n",
    "customer_dt=customer_dt[(customer_dt['TYPE_ID_Coverage_GSM'].isna()==False) & (customer_dt['TYPE_ID_Coverage_UMTS'].isna()==False) \n",
    "                        & (customer_dt['TYPE_ID_Coverage_LTE'].isna()==False)]\n",
    "\n",
    "customer_dt.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MCdzGKl5C_yu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As explained before, our original dataset includes **up to 3 months of telco consumption (voice, data and number of mobile lines) and expense** per customer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4EuXMN8GC_yv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX5**] [**CODE**] Create a new dataset named `customer_dt_summary` with only one register per `Company_id` and `Sub_product` category with the average value in `Data_usage`, `Voice_usage` and `Monthly_expense` and `N_lines`. Which is the size of your new dataset? Verify this new dataset does not contain **nulls**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Calcula la media de Data_usage, Voice_usage, Monthly_expense y N_lines para un solo registro por Company_id y Sub_product\n",
    "\n",
    "#customer_dt_summary = customer_dt[['Company_id','Sub_product','Data_usage', 'Voice_usage', 'Monthly_expense', 'N_lines']]\n",
    "#customer_dt_summary = customer_dt_summary.groupby([\"Company_id\", \"Sub_product\"]).mean()\n",
    "customer_dt_summary = customer_dt.groupby([\"Company_id\", \"Sub_product\"])[['Data_usage', 'Voice_usage', 'Monthly_expense', 'N_lines']].mean()\n",
    "\n",
    "print(customer_dt_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The resulted lenght is:\", len(customer_dt_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_-b0Xb3YC_yz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In Machine Learning, it is key to understand the nature of the data before training. For numeric variables, it is useful to calculate the distribution and main statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k-NHqupgC_y0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX6**] [**REPORT**] Calculate the main statistics (max, min, mean, median and standard deviation) of `Data_usage`, `Voice_usage`, `Monthly_expense` and `N_lines` variables. Plot a histogram for each of these variables\n",
    "\n",
    "Tip: use [Seaborn library](https://seaborn.pydata.org/) with `kde=False` to create a histogram. You also can use **dataframe_column.hist(bins=number_of_bins)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"*********** MAX ***********\")\n",
    "maxs = customer_dt[['Data_usage','Voice_usage','Monthly_expense','N_lines']].max()\n",
    "print(maxs)\n",
    "\n",
    "print(\"*********** MIN ***********\")\n",
    "mins = customer_dt[['Data_usage','Voice_usage','Monthly_expense','N_lines']].min()\n",
    "print(mins)\n",
    "\n",
    "print(\"*********** MEAN ***********\")\n",
    "mean = customer_dt[['Data_usage','Voice_usage','Monthly_expense','N_lines']].mean()\n",
    "print(mean)\n",
    "\n",
    "print(\"*********** MEDIAN ***********\")\n",
    "median = customer_dt[['Data_usage','Voice_usage','Monthly_expense','N_lines']].median()\n",
    "print(median)\n",
    "\n",
    "print(\"*********** STD ***********\")\n",
    "std = customer_dt[['Data_usage','Voice_usage','Monthly_expense','N_lines']].std(ddof=0)\n",
    "print(std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Histogrma tipo matlab\n",
    "customer_dt['Data_usage'].hist(bins=6,color='yellow',ec=\"black\",figsize=(3, 3))\n",
    "plt.show()\n",
    "\n",
    "customer_dt['Voice_usage'].hist(bins=6,color='yellow',ec=\"black\",figsize=(3, 3))\n",
    "plt.show()\n",
    "\n",
    "customer_dt['Monthly_expense'].hist(bins=6,color='yellow',ec=\"black\",figsize=(3,3))\n",
    "plt.show()\n",
    "\n",
    "customer_dt['N_lines'].hist(bins=6,color='yellow',ec=\"black\",figsize=(3,3))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3jMm8RXrC_y6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Additionaly to understanding each individual variable, it is important to understand how they are related to each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "64w-J0FPC_y9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX7**] [**REPORT**] Calculate and plot the correlation matrix between traffic attributes (i.e. `Voice_usage` and `Data_usage`), `Monthly_expense`, `N_lines`, `Turnover` and `CNT_EMPLOYEE`. \n",
    "- Which are the variables with more and less correlation with respect to the `Monthly_expense` variable?\n",
    "- Which are the top 2 variables with higher correlation with `Voice_usage`?\n",
    "- Is `Data_usage` correlated with `Turnover`? Does it mean that a company spend more in `Data usage` when its `Turnover`increases? \n",
    "- Which is the highest correlated variable with `CNT_EMPLOYEE`?\n",
    "- A company with high `Voice traffic` consumption but low `Data traffic`uses to spend more that other company with high `Data traffic`and low `Voice traffic`? Justify your answer.\n",
    "\n",
    "Tip: use [pandas.DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to compute a correlation matrix, and [matplotlib.pyplot.matshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.matshow.html) to show this graphically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Visualizamos primero los valores de correlacion\n",
    "customer_dt_atr=customer_dt[['Voice_usage','Data_usage','Monthly_expense', 'N_lines', 'Turnover', 'CNT_EMPLOYEE']]\n",
    "f = plt.figure(figsize=(19, 15))\n",
    "\n",
    "#Para la matriz de correlacion usamos lo que nos dice el tip\n",
    "plt.matshow(customer_dt_atr.corr(), fignum=f.number)\n",
    "plt.xticks(range(customer_dt_atr.shape[1]), customer_dt_atr.columns, fontsize=14, rotation=45)\n",
    "plt.yticks(range(customer_dt_atr.shape[1]), customer_dt_atr.columns, fontsize=14)\n",
    "cb = plt.colorbar()\n",
    "cb.ax.tick_params(labelsize=14)\n",
    "customer_dt_atr.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z7IzTAJTC_zB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another option to analyze the relation 1-to-1 between 2 variables in through scatter plots. Let's simplify the original dataset and create a new `training_dt`dataset with only `Voice_usage`, `Data_usage` and `Monthly_expense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sicrVeY1C_zB",
    "outputId": "27619a58-50aa-4359-fb69-915e7072f83b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Usariamos la primera linea que hemos usado antes, pero para menos variables\n",
    "training_dt=customer_dt_summary[['Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "training_dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHAKVxRMC_zE",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX8**] [**REPORT**] Visualize a scatter plot with `Voice_usage` vs `Monthly_expense` variables. Could you visually identify any cluster? How many? Repeat the plot with registers which `Voice_usage`is between 0 and 10000 minutes. Could you identify any cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = customer_dt.Voice_usage\n",
    "y = customer_dt.Monthly_expense\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Voice_usage')\n",
    "plt.ylabel('Monthly_expense')\n",
    "plt.show()\n",
    "plt.scatter(x,y)\n",
    "\n",
    "#Para hacer el zoom\n",
    "plt.ylim(0,3000)\n",
    "plt.xlim(0,10000)\n",
    "\n",
    "plt.xlabel('Voice_usage')\n",
    "plt.ylabel('Monthly_expense')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qOu4xIs9C_zG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX9**] [**REPORT**] Visualize a scatter plot with `Data_usage` vs `Monthly_expense` variables. Could you visually identify any cluster? How many?Repeat the plot with registers which `Data_usage`is between 0 and 500 GB. Could you identify any cluster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = customer_dt.Data_usage\n",
    "y = customer_dt.Monthly_expense\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Data_usage')\n",
    "plt.ylabel('Monthly_expense')\n",
    "plt.show()\n",
    "plt.scatter(x,y)\n",
    "\n",
    "plt.ylim(0,5000)\n",
    "plt.xlim(0,500)\n",
    "\n",
    "plt.xlabel('Data_usage')\n",
    "plt.ylabel('Monthly_expense')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAoUllbjC_zJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX10**] [**REPORT**] To improve our understanding of the data, plot a 3D visualization between `Voice_usage`, `Data_usage`and `Monthly_expense` for a new subset of the dataset where `Voice_usage`is below 10000 minutes and `Data_usage`is <=100 GB.\n",
    "- Could you visually identify any cluster? How many?\n",
    "- Could you identify any **outlier**?\n",
    "- Could you identify a cluster bigger than the others? Describe approximately it in terms of the values of these 3 variables\n",
    "\n",
    "\n",
    "Tip: use [scatter3d](https://matplotlib.org/3.1.1/gallery/mplot3d/scatter3d.html) to create 3D scatter plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we rotate the 3D scatter plot, the output looks like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UggmcRYWC_zK",
    "outputId": "8756f6a6-542a-4bc1-fa5d-61d16fade4df",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "training_dt=customer_dt_summary[['Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "#Hacemos un zoom de esta otra manera\n",
    "training_dt_filtered=training_dt[(training_dt['Voice_usage']<=10000) & (training_dt['Data_usage']<=100)]\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "X=training_dt_filtered.loc[:,'Voice_usage'].values\n",
    "Y=training_dt_filtered.loc[:,'Data_usage'].values\n",
    "Z=training_dt_filtered.loc[:,'Monthly_expense'].values\n",
    "\n",
    "ax.set_xlabel('Voice_usage')\n",
    "ax.set_ylabel('Data_usage')\n",
    "ax.set_zlabel('Monthly_expense')\n",
    "ax.scatter3D(X, Y, Z, c=Z, cmap='viridis', linewidth=5);\n",
    "#Angulo para visualizar\n",
    "ax.view_init(0, -60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ASoUeBhYC_zM",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If we rotate the 3D scatter plot, the output looks like as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "training_dt=customer_dt_summary[['Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "#Hacemos un zoom de esta otra manera\n",
    "training_dt_filtered=training_dt[(training_dt['Voice_usage']<=10000) & (training_dt['Data_usage']<=100)]\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "X=training_dt_filtered.loc[:,'Voice_usage'].values\n",
    "Y=training_dt_filtered.loc[:,'Data_usage'].values\n",
    "Z=training_dt_filtered.loc[:,'Monthly_expense'].values\n",
    "\n",
    "ax.set_xlabel('Voice_usage')\n",
    "ax.set_ylabel('Data_usage')\n",
    "ax.set_zlabel('Monthly_expense')\n",
    "ax.scatter3D(X, Y, Z, c=Z, cmap='viridis', linewidth=5);\n",
    "#Angulo para visualizar\n",
    "ax.view_init(90, -60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KwFlx-vBC_zP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 3: Training the model and performance evaluation: Segmentation of customers through K-means clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KYkcTTa4C_zQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Once the dataset has been processed and we have a first understanding of the type and characteristics of the variables, we are ready to apply clustering methods to group the register to answer the key marketing question: How are our customers?\n",
    "\n",
    "Firstly, we will code our own Kmeans algorithm. As the dataset has a high number of features, we will select voice usage, data usage and monthly expense variables to fit the clusters.\n",
    "Once the clustering is done, we need to understand the output. 2-dimension and 3-dimension scatter plot visualizations are excellent techniques to evaluate the clustering output.\n",
    "To check if our Kmeans algorithm works properly, we will use the Sklearn’s Kmeans function to cluster the dataset. We will compare the 2D and 3D plots from the Sklearn clustering and ours.\n",
    "Finally, as part of any Machine Learning Project, we need to calculate the perfomance of our model. For Kmeans, we will 1) estimate the optimal K value through the Elbow method and 2) calculate the sihouette score for several values of K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Cdo7H1eC_zQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Your own K-means function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1-1zZr5_C_zR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX11**] [**CODE**] Build a `calculate_distance` function to calculate the distance between each point and the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "96V_hgOOC_zS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Debido al enunciado no nos quedo muy claro si debiamso calcular la distancia para un solo punto, un solo centroide\n",
    "#o todos los puntos juntos, asi que para que fuera mas limpia la formula siguiente, consideramos que\n",
    "#recibimos todo el dataset como X y que centroid es todo los centroides, asi ponemos todo en conjunto\n",
    "def calculate_distance (X, centroid):\n",
    "    squareDistance=np.zeros((X.shape[0],centroid.shape[0]))\n",
    "    #Calculamos las distancias de cada punto del dataset a cada uno de los centroides con dos for\n",
    "    for i in range(len(X)): #Este for nos recorrera todo el dataset X\n",
    "        x_val = np.array(X[i,:])\n",
    "        for j in range(len(centroid)): #Este recorrera todos los centroides\n",
    "            #distancia al cuatrado seria\n",
    "            centroid_val = np.array(centroid[j,:])\n",
    "            vect = x_val - centroid_val\n",
    "            #la diferencia entre x y el centroide\n",
    "            squareDistance[i,j] = np.sqrt(np.dot(vect.T, vect)) #y luego la raiz de los cuadrados para obtener la distancia\n",
    "    return squareDistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qi1JkKiPC_zU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX12**] [**CODE**] Build `K_means_clustering` function that creates a clustering according to K-means methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QMOpImtiC_zU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances_argmin\n",
    "def K_means_clustering(X, n_clusters=2, seed=1, num_iterations=10):\n",
    "    # Initialitize centroids based on a random selection of #n_clusters samples of X \n",
    "    rng = np.random.RandomState(seed)\n",
    "    i = rng.permutation(X.shape[0])[:n_clusters] \n",
    "    #esta instruccion nos dio problemas y tuvimos que acceder a los valores de X con iloc, pero al parecer\n",
    "    #era un problema que surgia si no pasabamos el dataframe correctamente\n",
    "    centroids = X[i] \n",
    "    \n",
    "    #Repeat the process during num_iterations or convergence achieved\n",
    "    for num in range(0,num_iterations):\n",
    "    \n",
    "    #For each iteration, calculate the shortest distance of each point of X to centroids\n",
    "    #Labels are based on the index in the centroids array\n",
    "\n",
    "        #Tambien es posible usar la función pairwise_distances_argmin para calcular los indices \n",
    "        #de los centroides tal que la distancia a cada punto es minima, de hecho es una instruccion que viene\n",
    "        #importada por defecto\n",
    "        #labels = pairwise_distances_argmin(X, centroids)\n",
    "        \n",
    "        #También, y la opcion que tiene mas sentido, podemos usar la función que hemos creado anteriormente\n",
    "        distances_to_centroids = calculate_distance(X, centroids)\n",
    "        labels = np.argmin(distances_to_centroids, axis=1)\n",
    "        \n",
    "    #Calculate the new centroids based on the means of each point assigned to each cluster \n",
    "        \n",
    "        #Es aplicar el calculo de la media solo a las X que pertenerzcan a cierto label (numero de cluster)\n",
    "        new_centroids = np.array([X[labels == i].mean(0) for i in range(n_clusters)]) #asi tenemos los nuevos centroides\n",
    "           \n",
    "    # Evaluate convergence: if new_centroids=centroids, stop iterations\n",
    "    \n",
    "        if np.all(centroids == new_centroids):\n",
    "            #Se converge\n",
    "            print('Convergence achieved with:',num, 'iterations')\n",
    "            break #hacmeos break para detener el programa\n",
    "        else:\n",
    "            if num%10 == 0 and num != 0:\n",
    "                #no se converge\n",
    "                print('No convergence yet after', num, 'iterations')\n",
    "        centroids = new_centroids\n",
    "        \n",
    "    return centroids, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "weuHT--kC_zX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's define the `training_dt` dataset based on the following variables: `Voice_usage`, `Data_usage`, `Monthly_expense`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_kK5RkQMC_zY",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_dt=customer_dt_summary[['Voice_usage', 'Data_usage', 'Monthly_expense']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rIsXrFy6C_zb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX13**][**CODE**] Execute your `K_means_clustering` function to `training_dt` and number of clusters=3. Calculate the centroids of each cluster. How many iterations are necessary to achieve the clustering?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Importante usar training_dt.values y no training_dt para acceder a los valores del dataframe\n",
    "centroids, labels = K_means_clustering(training_dt.values, n_clusters=3, seed=1, num_iterations=100)\n",
    "\n",
    "print('Centroides de cada cluster:')\n",
    "centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y0LnAvJHC_zd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, it's time to understand how the clustering process works! To do it, we are plotting the `training_dt` painting the colour based on `Cluster_id`, output from the k-means. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k8jWJPNmC_ze",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " [**EX14**][**REPORT**] Plot the following scatter plots representing the centroids:\n",
    " - Between `Data_usage` vs `Voice_usage` \n",
    " - Between `Monthly_expense`vs `Voice_usage` and\n",
    " - Between `Monthly_expense`vs `Data_usage`\n",
    " \n",
    " Describe the minority cluster in terms of `Data_usage`, `Voice_usage`and `Monthly_expense`. How many register is if formed by?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "voice = training_dt.loc[:,'Voice_usage']\n",
    "data = training_dt.loc[:,'Data_usage']\n",
    "month_expense = training_dt.loc[:,'Monthly_expense']\n",
    "\n",
    "#Data_usage vs Voice_usage\n",
    "fig = plt.figure()\n",
    "plt.scatter(voice,data,c=labels,cmap='viridis')\n",
    "plt.xlabel('Voice_usage')\n",
    "plt.ylabel('Data_usage')\n",
    "plt.show()\n",
    "\n",
    "#Voice_usage vs Monthly_expense\n",
    "fig = plt.figure()\n",
    "plt.scatter(voice,month_expense,c=labels,cmap='viridis')\n",
    "plt.xlabel('Voice_usage')\n",
    "plt.ylabel('Monthly_expense')\n",
    "plt.show()\n",
    "\n",
    "#Data_usage vs Monthly_expense\n",
    "fig = plt.figure()\n",
    "plt.scatter(data,month_expense,c=labels,cmap='viridis')\n",
    "plt.xlabel('Data_usage')\n",
    "plt.ylabel('Monthly_expense')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zVF3eNuhC_zi",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX15**] [**CODE**] [**REPORT**] Execute the Sklearn library's KMeans function and compare both `Data_usage`vs `Voice_usage`scatter plots. Are they similar?\n",
    "\n",
    "Tip: We recommend the following  [KMeans()](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) parameters: `init`='random', `n_init`=10, `tol`=1e-04 and `random_state`=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Es como el ejercicio anterior, pero usamos el modelo KMeans de la libreria que hay importada por defecto\n",
    "kmeans = KMeans(n_clusters=3, init='random',n_init=10,tol=1e-04,random_state=0).fit(training_dt)\n",
    "labels = kmeans.predict(training_dt)\n",
    "\n",
    "voice = training_dt.loc[:,'Voice_usage']\n",
    "data = training_dt.loc[:,'Data_usage']\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(voice,data,c=labels)\n",
    "plt.xlabel('Voice_usage')\n",
    "plt.ylabel('Data_usage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xwv6EgeiC_zl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX16**] [**REPORT**] Repeat the 3D plot visualization between `Voice_usage`, `Data_usage`and `Monthly_expense` after the clustering process. Apply a rotation of (0, -60)\n",
    "\n",
    "Tip: use [scatter3d](https://matplotlib.org/3.1.1/gallery/mplot3d/scatter3d.html) to create 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Como en el tutorial y en los ejercicios anteriores\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "#A diferencia del ejercicio 10, ahora usamso el modelo KMeans para dibujar\n",
    "kmeans = KMeans(n_clusters=3, init='random',n_init=10,tol=1e-04,random_state=0).fit(training_dt)\n",
    "labels = kmeans.predict(training_dt)\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "X=training_dt.loc[:,'Voice_usage'].values\n",
    "Y=training_dt.loc[:,'Data_usage'].values\n",
    "Z=training_dt.loc[:,'Monthly_expense'].values\n",
    "\n",
    "ax.set_xlabel('Voice_usage')\n",
    "ax.set_ylabel('Data_usage')\n",
    "ax.set_zlabel('Monthly_expense')\n",
    "ax.scatter3D(X, Y, Z, c=labels, cmap='viridis', linewidth=5)\n",
    "ax.view_init(0, -60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xBDuranWC_zo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Measuring the quality of the clustering and the optimal K: Elbow method and sihouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HYHKv-2uC_zp",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The number of clusters to choose may not always be so obvious in real-world applications, especially if we are working with a higher dimensional dataset that cannot be visualized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvD4di53C_zq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX17**] [**REPORT**] Repeat the clustering (using Sklearn's or your own Kmeans function) with **K=5** and plot `Data_usage`vs `Voice_usage`scatter visualization. Can the new 5 clusters be visually distinguished? From a visual perspective, is this new cluster better than with **K=3**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=5, init='random',n_init=10,tol=1e-04,random_state=0).fit(training_dt)\n",
    "\n",
    "labels=kmeans.predict(training_dt)\n",
    "voice = training_dt.loc[:,'Voice_usage']\n",
    "data = training_dt.loc[:,'Data_usage']\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(voice,data,c=labels)\n",
    "plt.xlabel('Voice_usage')\n",
    "plt.ylabel('Data_usage')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GwqvSmUqC_zs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The elbow method is a useful graphical tool to estimate the optimal number of clusters. Intuitively, we can say that, if k increases, the distorsion within each cluster will decrease because the samples will be closer to their centroids. However, sometimes is not efficient to increase the **K** value because the distorsion doesn't decrease enough in comparision with the computation load required for higher **K**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z65jtHdFC_zt",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's calculate the Elbow method for the previous dataset, i.e. containing only `Data_usage` and `Voice_usage` variables.\n",
    "We use [km.inertia_](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) from the Sklearn library's KMeans to measure the sum of squared distances of samples to their closest cluster center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9y5QBO7PC_zu",
    "outputId": "ec10e7a8-4e86-445b-83fd-e12dbca36282",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Aqui usamos exactamente lo que nos daban por defecto, pero depende que compañero lo ejecutemos\n",
    "#nos salta un aviso raro, aun asi sigue funcionando, quizas sea por version del anaconda\n",
    "\n",
    "#Error\n",
    "#UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=2\n",
    "\n",
    "# Selection of the dataset\n",
    "training_dt=customer_dt_summary[['Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "inertia = []\n",
    "#Calculate the Kmeans from K=1 to 10\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=10,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(training_dt)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnpmBFCSC_zw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX18**] [**REPORT**] Repeat the Elbow method for a training dataset formed by `Voice_usage`and `Data_usage`only. Which is the optimal **K** value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Selection of the dataset\n",
    "training_dt=customer_dt_summary[['Voice_usage', 'Data_usage']]\n",
    "inertia = []\n",
    "#Calculate the Kmeans from K=1 to 10\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(\n",
    "        n_clusters=i, init='random',\n",
    "        n_init=10, max_iter=10,\n",
    "        tol=1e-04, random_state=0\n",
    "    )\n",
    "    km.fit(training_dt)\n",
    "    inertia.append(km.inertia_)\n",
    "\n",
    "# plot\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XbIgiCenC_z1",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Silhouette** is a metric to measure the *quality* of the clustering process. Clustering models with a high **Silhouette** are said to be dense, i.e. samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other. This measure has a range of [-1, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kotDCWoaC_z2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX19**] [**CODE**][**REPORT**]Calculate the `silhouette_score`value for a range of KMeans clusters from 2 to 7. The dataset to use is `training_dt`with the following variables: `Voice_usage`, `Data_usage` and `Monthly_expense`. Which is the value of **K** with better **Silhouette**?\n",
    "\n",
    "Tip: use [silhouette_score](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) to calculate the silhouette score and further information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEtT8JqCC_z5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For a visual understanding about each cluster, we can plot the silhouette score for each sample of the dataset. Execute the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLU_yMj-C_z6",
    "outputId": "e5ed185f-eaf3-4009-e259-08514fddbf18",
    "scrolled": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Selection of the dataset\n",
    "training_dt=customer_dt_summary[['Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "for j in range(2, 8):\n",
    "    n_clusters=j\n",
    "    km  = KMeans(j, random_state=10)\n",
    "    cluster_labels = km.fit_predict(training_dt)\n",
    "    silhouette_avg = silhouette_score(training_dt, cluster_labels)\n",
    "    \n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(training_dt, cluster_labels)\n",
    "    # Create a subplot with 1 row and 1 columns\n",
    "    fig, (ax1) = plt.subplots(1,1)\n",
    "    fig.set_size_inches(8, 5)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(training_dt) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    y_lower = 10\n",
    "    for i in range(j):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    " \n",
    "    ax1.set_title(\"The silhouette plot for the various clusters\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HFCIlp3_C_z8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Step 4: Insights generation: Understanding the clustering output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUG59j31C_z9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Thanks to **Elbow method** and the **Silhouette** metric, we can confirm that K-Means with **K=3** is good enough to cluster our customer base and generate insights for our marketing department."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QU5nmjfyC_z9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX20**][**REPORT**]Repeat the K-Means clustering with **K=3** for the `training_dt`formed by `CNT_EMPLOYEE`, `Turnover`, `Voice_usage`, `Data_usage`, `Monthly_expense`. For each cluster, calculate the **mean**, **standard deviation**, **min**, **max** for each variable.\n",
    "- Which is the cluster with the highest voice usage? \n",
    "- Which is the cluster with the highest data usage?\n",
    "- Customers in the cluster with bigger companies (i.e. bigger number or employes and turnover) use to spend more than the others customers?\n",
    "- As a part of the data scientist team, which is your recommended cluster of customers to sell a new mobile tariff with unlimited data traffic? And for a new mobile tariff with unlimited voice traffic? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Como el ejercicio 6\n",
    "training_dt=customer_dt[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, init='random',n_init=10,tol=1e-04,random_state=0).fit(training_dt)\n",
    "labels=kmeans.predict(training_dt)\n",
    "\n",
    "print(\"*********** MEAN ***********\")\n",
    "means = training_dt.groupby(labels)[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']].mean()\n",
    "print(means)\n",
    "\n",
    "print(\"*********** STD ***********\")\n",
    "std = training_dt.groupby(labels)[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']].std()\n",
    "print(std)\n",
    "\n",
    "print(\"*********** MIN ***********\")\n",
    "mins = training_dt.groupby(labels)[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']].min()\n",
    "print(mins)\n",
    "\n",
    "print(\"*********** MAX ***********\")\n",
    "maxs = training_dt.groupby(labels)[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']].max()\n",
    "print(maxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XKF9XfDWC_0A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Let's repeat step 3 and 4 with Mixture of Gaussians clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9-8DfhgTC_0A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As we know, there are other mechanisms to cluster a dataset. Let's test how Mixture of Gaussians function from sklearn library works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CUiaYTTJC_0B",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX21**][**CODE**][**REPORT**] Execute the Mixture of Gaussians function (with number of components=3) to `training_dt` dataset with `Voice_usage`, `Data_usage`and `Monthly_expense` variables. \n",
    "- Which is the size of each cluster? \n",
    "- Visualize the scatter plot between `Data_usage` vs `Voice_usage`. Is it similar to the resulting from K-Means and K=3?\n",
    "- Visualize the scatter plot between `Data_usage` vs `Monthly_expense`. Is it similar to the resulting from K-Means and K=3?\n",
    "- Visualize the scatter plot between `Voice_usage` vs `Monthly_expense`. Is it similar to the resulting from K-Means and K=3?\n",
    "\n",
    "Tip: You may use [GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) from Sklearn libray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_dt=customer_dt[['Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "\n",
    "#Usamos esta vez el modelo de Mixtura de Gaussiana\n",
    "gm = GaussianMixture(n_components=3, init_params=\"kmeans\", covariance_type=\"full\")\n",
    "gm.fit(training_dt)\n",
    "labels=gm.predict(training_dt)\n",
    "\n",
    "voice = training_dt.loc[:,'Voice_usage']\n",
    "data = training_dt.loc[:,'Data_usage']\n",
    "month_expense = training_dt.loc[:,'Monthly_expense']\n",
    "\n",
    "#Data_usage vs Voice_usage\n",
    "fig = plt.figure()\n",
    "plt.scatter(voice,data,c=labels,cmap='viridis')\n",
    "plt.xlabel('Voice_usage')\n",
    "plt.ylabel('Data_usage')\n",
    "plt.show()\n",
    "\n",
    "#Data_usage vs Monthly_expense\n",
    "fig = plt.figure()\n",
    "plt.scatter(voice,month_expense,c=labels,cmap='viridis')\n",
    "plt.xlabel('Voice_usage')\n",
    "plt.ylabel('Monthly_expense')\n",
    "plt.show()\n",
    "\n",
    "#Voice_usage vs Monthly_expense\n",
    "fig = plt.figure()\n",
    "plt.scatter(data,month_expense,c=labels,cmap='viridis')\n",
    "plt.xlabel('Data_usage')\n",
    "plt.ylabel('Monthly_expense')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TS33NV9vC_0E",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX22**][**REPORT**] Visualize the 3D plot  between `Voice_usage`, `Data_usage`and `Monthly_expense` after the clustering process. Apply a rotation of (0, -60)\n",
    "\n",
    "Tip: use [scatter3d](https://matplotlib.org/3.1.1/gallery/mplot3d/scatter3d.html) to create 3D scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#De nuevo es una copia de ejercicios anteriores\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "#Ahora usamos el modelo de Mixtura de Gaussiana\n",
    "gm = GaussianMixture(n_components=3, init_params=\"kmeans\", covariance_type=\"full\").fit(training_dt)\n",
    "labels = gm.predict(training_dt)\n",
    "\n",
    "# Data for a three-dimensional line\n",
    "X=training_dt.loc[:,'Voice_usage'].values\n",
    "Y=training_dt.loc[:,'Data_usage'].values\n",
    "Z=training_dt.loc[:,'Monthly_expense'].values\n",
    "\n",
    "ax.set_xlabel('Voice_usage')\n",
    "ax.set_ylabel('Data_usage')\n",
    "ax.set_zlabel('Monthly_expense')\n",
    "ax.scatter3D(X, Y, Z, c=labels, cmap='viridis', linewidth=5)\n",
    "ax.view_init(0, -60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FOzxAeJvC_0I",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX23**][**REPORT**] Evaluate the **Silhouette** metric for MoG with **number of components** from 2 to 7. Which is the number of cluster with the highest score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Igual que el ejercicio 19 de Silhouette, pero cambiamos el modelo\n",
    "# Selection of the dataset\n",
    "training_dt=customer_dt_summary[['Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "for j in range(2, 8):\n",
    "    n_clusters=j\n",
    "    #Esta es la unica diferencia, usamos el modelo de mixtura de Gaussiana\n",
    "    gm  = GaussianMixture(n_components=n_clusters, init_params=\"kmeans\", covariance_type=\"full\").fit(training_dt)\n",
    "    cluster_labels = gm.predict(training_dt)\n",
    "    silhouette_avg = silhouette_score(training_dt, cluster_labels)\n",
    "    \n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(training_dt, cluster_labels)\n",
    "    # Create a subplot with 1 row and 1 columns\n",
    "    fig, (ax1) = plt.subplots(1,1)\n",
    "    fig.set_size_inches(8, 5)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(training_dt) + (n_clusters + 1) * 10])\n",
    "    \n",
    "    y_lower = 10\n",
    "    for i in range(j):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    " \n",
    "    ax1.set_title(\"The silhouette plot for the various clusters\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Blie93uVC_0L",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX24**][**REPORT**] For n_components=3 and a dataset formed by `CNT_EMPLOYEE`, `Turnover`, `Voice_usage`, `Data_usage` and `Monthly_expense` variables, calculate the MoG clustering. For each cluster, calculate the **mean**, **standard deviation**, **min**, **max** for each variable.\n",
    "- Are these clusters similar to the resulting from KMeans=3?\n",
    "- Which is the cluster with the highest voice usage? \n",
    "- Which is the cluster with the highest data usage?\n",
    "- Customers in the cluster with bigger companies (i.e. bigger number or employes and turnover) use to spend more than the others customers?\n",
    "- As a part of the data scientist team, which is your recommended cluster of customers to sell a new mobile tariff with unlimited data traffic? And for a new mobile tariff with unlimited voice traffic? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Copia del ejercicio 6 y 20\n",
    "training_dt=customer_dt[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']]\n",
    "\n",
    "gm = GaussianMixture(n_components=3, init_params=\"kmeans\", covariance_type=\"full\").fit(training_dt)\n",
    "labels=gm.predict(training_dt)\n",
    "\n",
    "print(\"*********** MEAN ***********\")\n",
    "means = training_dt.groupby(labels)[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']].mean()\n",
    "print(means)\n",
    "\n",
    "print(\"*********** STD ***********\")\n",
    "std = training_dt.groupby(labels)[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']].std()\n",
    "print(std)\n",
    "\n",
    "print(\"*********** MIN ***********\")\n",
    "mins = training_dt.groupby(labels)[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']].min()\n",
    "print(mins)\n",
    "\n",
    "print(\"*********** MAX ***********\")\n",
    "maxs = training_dt.groupby(labels)[['CNT_EMPLOYEE', 'Turnover', 'Voice_usage', 'Data_usage', 'Monthly_expense']].max()\n",
    "print(maxs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p7hFZI2WDtqh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[**EX25**][**REPORT**] In this project we have used several clustering techniques to segment a customer base with aim to have a clear understanding about how they are. It is a very useful application in marketing to identify clusters or groups of target customers to sell a product or to focus a marketing campaign. \n",
    "However, clustering have a lot of applications. In this exercise we will apply to other use case:\n",
    "\n",
    "We have clustered the population in an area of Barcelona according to several criterias:\n",
    "\n",
    "**Clustering execution 1:** is based on `number_family_members`, `electricity_consumption`and `water_consumption`. The output is the following:\n",
    "\n",
    "![<PJ1_clustering_1_plot>](PJ1_clustering_1.png)\n",
    "\n",
    "**Clustering execution 2:** is based on `number_pets`, `size_home`and `number_kids`. The output is the following:\n",
    "![<PJ1_clustering_2_plot>](PJ1_clustering_2.png)\n",
    "\n",
    "**Clustering execution 3:** is based on `education_level`, `salary`and `number_vehicles`. The output is the following:\n",
    "![<PJ1_clustering_3_plot>](PJ1_clustering_3.png)\n",
    "\n",
    "The caracteristic (average+deviation) of each variable that describes each segment (or cluster) for each clustering process is the following:\n",
    "![<clustering_summary_plot>](clustering_summary.png)\n",
    "\n",
    "\n",
    "\n",
    "Answer the following questions considering the previous maps and the description of each cluster (table above):\n",
    "- Describe the homes located in the map:\n",
    "![<homes_map_plot>](homes_map.png)\n",
    "- Which clustering execution has produced more homogeneous groups? Justify your answer\n",
    "- Which clustering execution should have better silhoette? Justify your anwer\n",
    "- If we would like to build a **fast food** restaurant, where would you place it? Justify your answer\n",
    "- If we would like to place a **school**, where would you place it? Justify your answer\n",
    "- In case we would like to place a **laundry**, which 3 variables you would select from the following dataset to launch a new clustering process:\n",
    "`number_family_members`, `monthly_dress_expense`, `number_shoes`, `electricity_consumption`,`number_cars`,`monthly_mobile_expense`, `number_rooms`, `water_consumption`, `number_pets`, `size_home`, `number_kids`, `education_level`, `salary` and `number_vehicles`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xj1-ZEjOC_0N",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Deliver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pQTE_WVQC_0O",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Deliver:\n",
    "\n",
    "* A zip file containing your notebook (.ipynb file) with all the [**CODE**] parts implemented.\n",
    "* A 5-pages PDF report including all parts of this notebook marked with \"[**REPORT**]\"\n",
    "\n",
    "The report should end with the following statement: **We hereby declare that, except for the code provided by the course instructors, all of our code, report, and figures were produced by ourselves.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uf0ouBNuC_0O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "KwFlx-vBC_zP",
    "3Cdo7H1eC_zQ",
    "xBDuranWC_zo",
    "HFCIlp3_C_z8",
    "XKF9XfDWC_0A",
    "xj1-ZEjOC_0N"
   ],
   "name": "P01_Clustering.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}